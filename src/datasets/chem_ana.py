import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import umap

def visualize_kmeans_pca_with_labels(df: pd.DataFrame, 
                                     target_columns: list, 
                                     n_clusters: int, 
                                     exclude_ids = None,
                                     id_column: str = None):
    """
    k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€PCAã§2æ¬¡å…ƒã«å¯è¦–åŒ–ã™ã‚‹ã€‚
    ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«IDãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        target_columns (list): ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆã€‚
        n_clusters (int): ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã®æ•°ã€‚
        id_column (str, optional): ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹IDã‚«ãƒ©ãƒ åã€‚Defaults to None.
    """
    print("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

    # 0. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æ¬ æå€¤ã®å‡¦ç†
    # ----------------------------------------------------------------------
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¤‰æ›´ã—ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚
    df_work = df.copy()
    # è¨ˆç®—å¯¾è±¡ã®ã‚«ãƒ©ãƒ ã«æ¬ æå€¤(NaN)ãŒã‚ã‚‹è¡Œã‚’å‰Šé™¤ã—ã¾ã™ã€‚
    df_work.dropna(subset=target_columns, inplace=True)

    if exclude_ids != None:
        mask = ~df_work['crop-id'].isin(exclude_ids)
        df_work = df_work[mask]

    if df_work.empty:
        print("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        return

    # 1. å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã¨å‰å‡¦ç†ï¼ˆæ¨™æº–åŒ–ï¼‰
    # ----------------------------------------------------------------------
    print("1. ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ã‚’è¡Œã£ã¦ã„ã¾ã™...")
    data_to_cluster = df_work[target_columns].values
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_to_cluster)

    # 2. k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    # ----------------------------------------------------------------------
    print(f"2. k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™... (ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters})")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
    labels = kmeans.fit_predict(data_scaled)

    # 3. ä¸»æˆåˆ†åˆ†æ (PCA) ã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
    # ----------------------------------------------------------------------
    print("3. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã§2æ¬¡å…ƒã«å‰Šæ¸›ã—ã¦ã„ã¾ã™...")
    reducer = PCA(n_components=2)
    from sklearn.manifold import TSNE
    #reducer = TSNE(n_components=2, perplexity=20, random_state=42, init = 'random')
    #reducer = umap.UMAP(n_components=2, random_state=42)
    principal_components = reducer.fit_transform(data_scaled)

    # PCAã®çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    pca_df = pd.DataFrame(data=principal_components, columns=["PC1", "PC2"])
    pca_df["cluster"] = labels
    # id_columnãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€IDã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
    if id_column:
        pca_df[id_column] = df_work[id_column].values

    pca_df[target_features] = df_work[target_columns].values

    # 4. å¯è¦–åŒ–
    # ----------------------------------------------------------------------
    print("4. çµæœã‚’ã‚°ãƒ©ãƒ•ã«æç”»ã—ã¦ã„ã¾ã™...")
    plt.style.use('seaborn-v0_8-whitegrid')
    # figã¨axã‚’å–å¾—ã—ã¦ã€ã‚ˆã‚Šè©³ç´°ãªæç”»è¨­å®šã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    fig, ax = plt.subplots(figsize=(16, 10))

    sns.scatterplot(
        x="PC1",
        y="PC2",
        hue="cluster",
        palette=sns.color_palette("hsv", n_clusters),
        data=pca_df,
        legend="full",
        alpha=0.8,
        ax=ax
    )

    # â˜…â˜…â˜… IDãƒ©ãƒ™ãƒ«ã‚’æç”»ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ  â˜…â˜…â˜…
    if id_column:
        print("  -> å„ãƒ—ãƒ­ãƒƒãƒˆã«IDãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™...")
        # pca_dfã®å„è¡Œã«å¯¾ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
        for index, row in pca_df.iterrows():
            ax.text(
                x=row['PC1'] + 0.02, # Xåº§æ¨™ï¼ˆå°‘ã—å³ã«ãšã‚‰ã™ï¼‰
                y=row['PC2'] + 0.02, # Yåº§æ¨™ï¼ˆå°‘ã—ä¸Šã«ãšã‚‰ã™ï¼‰
                s=row[id_column],    # è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆï¼ˆIDï¼‰
                fontdict={'size': 8} # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
            )

    ax.set_title(f"k-means Clustering Results (k={n_clusters}) (PCA visualization)", fontsize=16)
    ax.set_xlabel("Principal Component 1 (PC1)", fontsize=12)
    ax.set_ylabel("Principal Component 2 (PC2)", fontsize=12)
    ax.legend(title="Cluster")
    plt.show()
    print("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    return reducer, pca_df



def analyze_factor_loadings(pca_model: PCA, feature_names: list):
    """
    PCAãƒ¢ãƒ‡ãƒ«ã®å› å­è² è·é‡ã‚’åˆ†æã—ã€çµæœã‚’è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        pca_model (PCA): fitæ¸ˆã¿ã®PCAãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        feature_names (list): PCAã«ä½¿ç”¨ã—ãŸå…ƒã®ç‰¹å¾´é‡ï¼ˆã‚«ãƒ©ãƒ ï¼‰åã®ãƒªã‚¹ãƒˆã€‚
    """
    print("\n--- å› å­è² è·é‡ã®åˆ†æçµæœ ---")
    
    # å› å­è² è·é‡ã‚’å–å¾—
    # pca_model.components_ ã«ã¯ã€å„ä¸»æˆåˆ†ãŒã©ã®å…ƒç‰¹å¾´é‡ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã‹ã®æƒ…å ±ãŒå…¥ã£ã¦ã„ã‚‹
    loadings = pca_model.components_

    # åˆ†ã‹ã‚Šã‚„ã™ã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    # è¡Œã«ä¸»æˆåˆ†(PC1, PC2)ã€åˆ—ã«å…ƒã®ç‰¹å¾´é‡åã‚’è¨­å®š
    loadings_df = pd.DataFrame(
        loadings,
        columns=feature_names,
        index=[f'PC{i+1}' for i in range(loadings.shape[0])]
    )
    
    print("å„ä¸»æˆåˆ†ï¼ˆPC1, PC2ï¼‰ãŒã€ã©ã®å…ƒã®ç‰¹å¾´é‡ã¨é–¢é€£ãŒå¼·ã„ã‹ã‚’ç¤ºã—ã¾ã™ã€‚")
    print("å€¤ã®çµ¶å¯¾å€¤ãŒå¤§ãã„ã»ã©ã€ãã®ä¸»æˆåˆ†ã«å¯¾ã™ã‚‹å½±éŸ¿åŠ›ãŒå¤§ãã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n")
    
    # çµæœã‚’è¡¨ç¤º
    print(loadings_df)

    # å„ä¸»æˆåˆ†ã®è§£é‡ˆã‚’è£œåŠ©ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    for i, pc in enumerate(loadings_df.index):
        # ä¿‚æ•°ã®çµ¶å¯¾å€¤ãŒå¤§ãã„é †ã«ç‰¹å¾´é‡ã‚’ã‚½ãƒ¼ãƒˆ
        sorted_features = loadings_df.loc[pc].abs().sort_values(ascending=False)
        top_feature = sorted_features.index[0]
        top_value = loadings_df.loc[pc, top_feature]
        
        direction = "æ­£" if top_value > 0 else "è² "
        
        print(f"\nè€ƒå¯Ÿ: {pc} ã¯ã€ç‰¹ã« '{top_feature}' ã®ç‰¹å¾´é‡ã¨å¼·ã„{direction}ã®ç›¸é–¢ãŒã‚ã‚Šã¾ã™ ({top_value:.2f})ã€‚")
        print(f"ã¤ã¾ã‚Šã€{pc}è»¸ã®ã‚¹ã‚³ã‚¢ãŒé«˜ã„ãƒ‡ãƒ¼ã‚¿ã¯ã€'{top_feature}' ã®å€¤ãŒ{'é«˜ã„ï¼ˆä½ã„ï¼‰' if top_value > 0 else 'ä½ã„ï¼ˆé«˜ã„ï¼‰'}å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity


def plot_kde_pairplot(df: pd.DataFrame, columns: list):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨3ã¤ä»¥ä¸Šã®ã‚«ãƒ©ãƒ åã‚’æŒ‡å®šã—ã¦ã€KDEãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

    Args:
        df (pd.DataFrame): åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        columns (list): åˆ†æå¯¾è±¡ã®ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆï¼ˆ2ã¤ä»¥ä¸Šï¼‰ã€‚
    """
    # --- 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ ---
    if len(columns) < 2:
        print("ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ©ãƒ ã¯2ã¤ä»¥ä¸ŠæŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    for col in columns:
        if col not in df.columns:
            print(f"ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ©ãƒ  '{col}' ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return

    # --- 2. ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ– ---
    # åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’æŠ½å‡º
    data_to_scale = df[columns]
    
    # StandardScalerã‚’ä½¿ã£ã¦æ¨™æº–åŒ–
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data_to_scale)
    
    # æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ ¼ç´
    # ã‚«ãƒ©ãƒ åã‚‚ã‚ã‹ã‚Šã‚„ã™ã "_scaled" ã‚’ã¤ã‘ã¾ã™
    scaled_df = pd.DataFrame(scaled_data, columns=[f"{c}_scaled" for c in columns])

    # --- 3. Seaborn PairGridã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒƒãƒˆ ---
    print("ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆä¸­ã§ã™... ã“ã‚Œã«ã¯å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
    
    # PairGridã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    g = sns.PairGrid(scaled_df)
    
    # ä¸ŠåŠåˆ†ã«æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    # s=10ã§ãƒãƒ¼ã‚«ãƒ¼ã‚µã‚¤ã‚ºã‚’å°ã•ãã€alpha=0.6ã§é€æ˜åº¦ã‚’è¨­å®š
    g.map_upper(sns.scatterplot, s=10, alpha=0.6)
    
    # å¯¾è§’ç·šã«1æ¬¡å…ƒã®KDEãƒ—ãƒ­ãƒƒãƒˆ
    g.map_diag(sns.kdeplot, lw=3) # lw=3ã§ç·šã®å¤ªã•ã‚’è¨­å®š
    
    # ä¸‹åŠåˆ†ã«2æ¬¡å…ƒã®KDEãƒ—ãƒ­ãƒƒãƒˆï¼ˆç­‰é«˜ç·šï¼‰
    g.map_lower(sns.kdeplot, fill=True) # fill=Trueã§å¡—ã‚Šã¤ã¶ã—

    # ã‚°ãƒ©ãƒ•å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ 
    g.fig.suptitle('KDE Pair Plot of Variables', y=1.02, fontsize=16)

    plt.show()

def calculate_and_save_density(df: pd.DataFrame, columns: list, id_column: str, output_filename: str):
    """
    å„ãƒ‡ãƒ¼ã‚¿ã®å¯†åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€IDã¨ç´ã¥ã‘ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

    Args:
        df (pd.DataFrame): åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        columns (list): åˆ†æå¯¾è±¡ã®ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆã€‚
        id_column (str): å„ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹IDã‚«ãƒ©ãƒ åã€‚
        output_filename (str): ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: 'density_results.csv')ã€‚
    """
    # --- 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ ---
    if id_column not in df.columns:
        print(f"ã‚¨ãƒ©ãƒ¼: IDã‚«ãƒ©ãƒ  '{id_column}' ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return
    
    for col in columns:
        if col not in df.columns:
            print(f"ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ©ãƒ  '{col}' ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return

    # --- 2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æ¨™æº–åŒ– ---
    print("ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ã¨ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    data_to_process = df[columns].values
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data_to_process)

    # --- 3. ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ---
    # bandwidthã¯ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„
    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(scaled_data)

    # --- 4. å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å¯†åº¦ã‚’è¨ˆç®— ---
    # score_samplesã¯å¯¾æ•°å¯†åº¦ã‚’è¿”ã™ãŸã‚ã€np.exp()ã§å…ƒã®å¯†åº¦ã«æˆ»ã—ã¾ã™
    log_density = kde.score_samples(scaled_data)
    density_scores = np.exp(log_density)
    print("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å¯†åº¦è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- 5. çµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ ---
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¿…è¦ãªåˆ—ã‚’ã‚³ãƒ”ãƒ¼
    result_df = df[[id_column] + columns].copy()
    
    # æ–°ã—ã„åˆ—ã¨ã—ã¦å¯†åº¦ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
    result_df['density_score'] = density_scores
    
    # --- 6. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ ---
    try:
        result_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        print(f"çµæœã‚’ '{output_filename}' ã¨ã—ã¦æ­£å¸¸ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return result_df

import os
import itertools

def analyze_and_plot(df: pd.DataFrame, variable_columns: list, label_column: str, output_folder: str, id_column: str = None):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å—ã‘å–ã‚Šã€å„ç¨®ãƒ—ãƒ­ãƒƒãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚
    æ•£å¸ƒå›³ã«ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§IDãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã§ãã‚‹ã€‚
    """
    print("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    os.makedirs(output_folder, exist_ok=True)
    print(f"ã‚°ãƒ©ãƒ•ã¯ '{output_folder}' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")
    if id_column:
        print(f"IDã‚«ãƒ©ãƒ ã€Œ{id_column}ã€ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚æ•£å¸ƒå›³ã«IDã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

    data_counts = df[label_column].value_counts().to_dict()
    
    # --- 1. ãƒ©ãƒ™ãƒ«ã”ã¨ã®ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— & æ•£å¸ƒå›³ï¼‰ã®ä½œæˆã¨ä¿å­˜ ---
    print("\n## ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ãƒ™ãƒ«ã”ã¨ã®ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆãƒ»ä¿å­˜ã—ã¾ã™ã€‚ ##")
    unique_labels = df[label_column].unique()

    for label in unique_labels:
        print(f"\n-- ãƒ©ãƒ™ãƒ«ã€Œ{label}ã€ã®å‡¦ç†ã‚’é–‹å§‹ --")
        
        n_samples = data_counts.get(label, 0)
        if n_samples <= 2:
            print(f" -> âš ï¸ ãƒ‡ãƒ¼ã‚¿æ•°ãŒ {n_samples} å€‹ã®ãŸã‚ã€ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        else:
            label_dir = os.path.join(output_folder, label) 
            os.makedirs(label_dir, exist_ok=True)

        subset_df = df[df[label_column] == label]
        
        # 1-1. ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ
        correlation_matrix = subset_df[variable_columns].corr()
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)
        plt.title(f'crrelation of {label} (n={n_samples})', fontsize=16)
        filename = f'correlation_heatmap_{label}.png'
        filepath = os.path.join(output_folder, filename)
        plt.savefig(filepath, bbox_inches='tight')
        print(f" -> âœ… ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ '{filepath}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
        plt.close()

        # 1-2. 2å¤‰æ•°ã”ã¨ã®æ•£å¸ƒå›³ã‚’ä½œæˆ
        print(f" -> æ•£å¸ƒå›³ã‚’ä½œæˆä¸­...")
        for var1, var2 in itertools.combinations(variable_columns, 2):
            plt.figure(figsize=(8, 6))
            plt.scatter(subset_df[var1], subset_df[var2], alpha=0.7)
            
            # << è¿½åŠ : IDã‚«ãƒ©ãƒ ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å„ç‚¹ã«IDãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ >>
            if id_column and id_column in df.columns:
                for _, row in subset_df.iterrows():
                    # ãƒ†ã‚­ã‚¹ãƒˆãŒç‚¹ã«é‡ãªã‚Šã™ããªã„ã‚ˆã†ã«å°‘ã—èª¿æ•´
                    plt.text(x=row[var1], y=row[var2], s=str(row[id_column]), 
                             fontdict={'size': 7, 'color': 'darkslategray'},
                             ha='left', va='bottom')

            plt.title(f'ã€Œ{label}ã€: {var1} vs {var2} (n={n_samples})', fontsize=14)
            plt.xlabel(var1)
            plt.ylabel(var2)
            plt.grid(True)
            
            safe_var1 = str(var1).replace('(','').replace(')','').replace('/','')
            safe_var2 = str(var2).replace('(','').replace(')','').replace('/','')
            filename = f'scatterplot_{safe_var1}_vs_{safe_var2}.png'
            filepath = os.path.join(label_dir, filename)
            plt.savefig(filepath, bbox_inches='tight')
            plt.close()
        print(f" -> âœ… æ•£å¸ƒå›³ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    print("-" * 40)

    # --- 2. å„å¤‰æ•°ã®ç®±ã²ã’å›³ï¼ˆå¤‰æ•°ã”ã¨ã«åˆ†å‰²ï¼‰ã®ä½œæˆã¨ä¿å­˜ ---
    print("\n## ã‚¹ãƒ†ãƒƒãƒ—2: å¤‰æ•°ã”ã¨ã®ç®±ã²ã’æ¯”è¼ƒå›³ï¼ˆåˆ†å‰²ä¿å­˜ï¼‰ã‚’ä½œæˆãƒ»ä¿å­˜ã—ã¾ã™ã€‚ ##")
    counts_str = ", ".join([f"{name}: {count}" for name, count in data_counts.items()])
    
    for variable in variable_columns:
        plt.figure(figsize=(8, 6))
        sns.boxplot(x=label_column, y=variable, data=df)
        plt.title(f'{variable} : (num{counts_str})', fontsize=16)
        plt.tight_layout()
        
        safe_variable = str(variable).replace('(','').replace(')','').replace('/','')
        filename = f'boxplot_comparison_{safe_variable}.png'
        filepath = os.path.join(output_folder, filename)
        plt.savefig(filepath, bbox_inches='tight')
        print(f" -> âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filepath}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
        plt.close()
    
    print("\nã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ğŸ‰")



def classify_crop(crop_name):
  """
  ä½œç‰©ã®åå‰ã‚’å—ã‘å–ã‚Šã€ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™é–¢æ•°
  - 'Rice' -> 'Rice'
  - 'Pear', 'Appl' -> 'Fruit'
  - ãã‚Œä»¥å¤– -> 'Vegetable'
  """
  if crop_name == 'Rice':
    return 'Rice'
  elif crop_name in ['Pear', 'Appl']:
    return 'Fruit'
  else:
    return 'Vegetable'

if __name__ == '__main__':
    df = pd.read_excel('C:\\Users\\asahi\\Agri_Chemical_NN\\data\\raw\\riken\\chem_data.xlsx')
    #df = pd.read_excel('/home/nomura/Agri_Chemical_NN/data/raw/DRA015491/chem_data.xlsx')

    # ä»Šå›ã¯4ã¤ã®ç‰¹å¾´é‡ã™ã¹ã¦ã‚’ä½¿ã„ã¾ã™ã€‚
    target_features = [
        'pH',
        'EC',
        'Available.P',
        'NO3.N',
        'NH4.N',
        'Exchangeable.K',
        #'EC_ene'
    ]
    #target_features = ['pH_dry_soil', 'EC_electric_conductivity', 'Total_C', 'Total_N', 'available_P']


    #df['EC_ene'] = df['NO3.N'] + df['NH4.N'] + df['Exchangeable.K'] 

    

    exclude_ids = ['042_20_Sait_Eggp',
                   '235_21_Miyz_Spin', '360_22_Miee_Soyb', '121_20_Miyz_Spin', '125_20_Miyz_Spin',
                   '273_22_Naga_Rice', '334_22_Yama_Rice'
                   ]

    if exclude_ids != None:
        mask = ~df['crop-id'].isin(exclude_ids)
        df = df[mask]

        
    #df['category'] = df['crop'].apply(classify_crop)
    df['a'] = 'a'

    label = 'a'

    id_column = None

    analyze_and_plot(df = df, variable_columns = target_features, label_column = label, output_folder = f'/home/nomura/Agri_Chemical_NN/datas/{label}', id_column = id_column)

    # 3. ä½œæˆã—ãŸé–¢æ•°ã‚’å®Ÿè¡Œï¼
    # reduce_model, results = visualize_kmeans_pca_with_labels(df, target_features, 4,  
    #                                  exclude_ids,
    #                      'crop-id'
    #                      )
    
    # analyze_factor_loadings(reduce_model, target_features)
    #results.to_csv(f'/home/nomura/Agri_Chemical_NN/datas/pca_result_{target_features}.csv')

    #plot_kde_pairplot(df=df, columns=target_features)
    #kde = calculate_and_save_density(df=df, columns=target_features, id_column='crop-id', output_filename='/home/nomura/Agri_Chemical_NN/datas/kde.csv')
    #kde.to_csv('/home/nomura/Agri_Chemical_NN/datas/kde.csv')

